{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d86f6cd",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/VectorInstitute/rag-bootcamp/blob/uv-migration/implementations/document_search/document_search_llamaindex.ipynb)\n",
    "\n",
    "# Document Search with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168e6b6",
   "metadata": {},
   "source": [
    "This example shows how to use the Python [LlamaIndex](https://docs.llamaindex.ai/en/stable/) library to run a text-generation request on open-source LLMs and embedding models using the OpenAI SDK, then augment that request using the text stored in a collection of local PDF documents.\n",
    "\n",
    "### üìù Requirements\n",
    "\n",
    "To run this notebook, you will need:\n",
    "\n",
    "- **OpenAI API key**:  \n",
    "    - Sign up at [OpenAI](https://platform.openai.com/) and create an API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4da1f",
   "metadata": {},
   "source": [
    "## Set up the RAG workflow environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1801b",
   "metadata": {},
   "source": [
    "#### Install libraries (Only in Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ffdaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if 'COLAB_RELEASE_TAG' in os.environ:\n",
    "    # This is a Google Colab environment\n",
    "    \n",
    "    # Check if the notebook is running in a GPU environment and install the appropriate version of faiss\n",
    "    if 'COLAB_GPU' in os.environ:\n",
    "        !pip3 install faiss-gpu\n",
    "    else:\n",
    "        !pip3 install faiss-cpu\n",
    "\n",
    "    # Install other dependencies\n",
    "    !pip3 install llama-index llama-index-core llama-index-embeddings-huggingface llama-index-llms-openai-like llama-index-vector-stores-faiss # aieng-rag-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ad1b0-76d3-4db8-9705-a4a4ac56ebca",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a750497-5fc0-4a7f-8ed6-931c5d8759d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f637730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import os\n",
    "\n",
    "from aieng.rag.utils import get_device_name\n",
    "from aieng.rag.utils.search import DocumentReader, pretty_print, download_file\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, Settings, StorageContext\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d9175-73d5-4ef0-965b-acaa3fb4a91c",
   "metadata": {},
   "source": [
    "#### Load OpenAI env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96933bd8-4158-4c50-ad05-2a134d490fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\",\"https://api.openai.com/v1\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d4a0d",
   "metadata": {},
   "source": [
    "#### Download source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_PATH = \"./source_documents\"\n",
    "DOCUMENT_URL = \"https://vectorinstitute.ai/wp-content/uploads/2023/05/vector-institute-2021-22-annual-report_accessible.pdf\"\n",
    "\n",
    "download_file(DOCUMENT_URL, DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3d1c8-07cb-4e1a-88ac-087536c6e96e",
   "metadata": {},
   "source": [
    "#### Choose LLM and embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a10552-cb1a-4088-9081-05494fca9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_MODEL_NAME = \"gpt-4.1\"\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-base-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e558afb",
   "metadata": {},
   "source": [
    "## Start with a basic generation request without RAG augmentation\n",
    "\n",
    "Let's start by asking the model a difficult, domain-specific question we don't expect it to have an answer to. A simple question like \"*What is the capital of France?*\" is not a good question here, because that's world knowledge that we expect the LLM to know.\n",
    "\n",
    "Instead, we want to ask it a question that is domain-specific and it won't know the answer to. A good example would be an obscure detail buried deep within a company's annual report. For example:\n",
    "\n",
    "*How many Vector scholarships in AI were awarded in 2022?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6133a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many Vector scholarships in AI were awarded in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a22c5",
   "metadata": {},
   "source": [
    "## Now send the query to the open source model using KScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3d559a-74cf-4406-9ee4-61944f3e4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "\n",
      "assistant: In 2022, the **Vector Institute awarded 178 Vector Scholarships in Artificial Intelligence (VSAI)** to students beginning eligible AI-related master‚Äôs programs at Ontario universities. \n",
      "\n",
      "**Source:**  \n",
      "- [Vector Institute 2022 Annual Report](https://vectorinstitute.ai/wp-content/uploads/2023/06/Vector-Annual-Report-2022.pdf) (see page 18)\n",
      "- [Vector Institute News Release, September 2022](https://vectorinstitute.ai/vector-institute-awards-178-vector-scholarships-in-artificial-intelligence-to-incoming-masters-students/)\n",
      "\n",
      "Let me know if you need more details!\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAILike(\n",
    "    model=GENERATOR_MODEL_NAME,\n",
    "    is_chat_model=True,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    api_base=OPENAI_BASE_URL,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "message = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=query\n",
    "    )\n",
    "]\n",
    "try:\n",
    "    result = llm.chat(message)\n",
    "    print(f\"Result: \\n\\n{result}\")\n",
    "except Exception as err:\n",
    "    if \"Error code: 503\" in err.message:\n",
    "        print(f\"The model {GENERATOR_MODEL_NAME} is not ready yet.\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1c200",
   "metadata": {},
   "source": [
    "Without additional information, the model is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022.** Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the `source_documents` folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255ea68",
   "metadata": {},
   "source": [
    "## Ingestion: Load and store the documents from `source_documents`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d0304",
   "metadata": {},
   "source": [
    "Start by reading in all the PDF files from `source_documents`, break them up into smaller digestible chunks, then encode them as vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5710c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source documents: 42\n",
      "Number of text chunks: 196\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY_PATH = \"./source_documents\"\n",
    "\n",
    "# Load PDFs\n",
    "doc_reader = DocumentReader(directory_path=DIRECTORY_PATH ,create_nodes=True)\n",
    "docs, chunks = doc_reader.load()\n",
    "\n",
    "print(f\"Number of source documents: {len(docs)}\")\n",
    "print(f\"Number of text chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7545e",
   "metadata": {},
   "source": [
    "#### Define the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "268ab345-4676-4700-8965-4639751e7fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the embeddings model...\n"
     ]
    }
   ],
   "source": [
    "device = get_device_name()\n",
    "\n",
    "print(f\"Setting up the embeddings model...\")\n",
    "embeddings = HuggingFaceEmbedding(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    device=device,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ed121-6e4c-46e4-926c-33aa6ee77759",
   "metadata": {},
   "source": [
    "#### Set LLM and embedding model [recommended for LlamaIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7446327c-d8b9-4928-92c7-fb0af4fb0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5050576-073c-4615-9621-b6b217a13b0e",
   "metadata": {},
   "source": [
    "## Retrieval: Make the document chunks available via a retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0131a2d-4cd6-4c1e-835e-540329fda2b2",
   "metadata": {},
   "source": [
    "The retriever will identify the document chunks that most closely match our original query. (This takes about 1-2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c49d0093-0105-499a-a7e3-ebf6326a85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_model_dim(embed_model):\n",
    "    embed_out = embed_model.get_text_embedding(\"Dummy Text\")\n",
    "    return len(embed_out)\n",
    "\n",
    "faiss_dim = get_embed_model_dim(embeddings)\n",
    "faiss_index = faiss.IndexFlatL2(faiss_dim)\n",
    "\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex(chunks, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37f512cb-36f8-4afb-a8c6-0c187a0d9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "# Retrieve the most relevant context from the vector store based on the query\n",
    "retrieved_docs = retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68093b7f-4da7-45d8-8a58-536fb7f8aa5c",
   "metadata": {},
   "source": [
    "Let's see what results it found. Important to note, these results are in the order the retriever thought were the best matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43ff6d3c-b6e8-4702-8591-44e0d7b7d484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "26 \n",
      " \n",
      " \n",
      "VECTOR SCHOLARSHIPS IN \n",
      "AI ATTRACT TOP TALENT \n",
      "TO ONTARIO UNIVERSITIES \n",
      "109 \n",
      "Vector Scholarships in AI awarded \n",
      "34 \n",
      "Programs \n",
      "13 \n",
      "Universities \n",
      "351 \n",
      "Scholarships awarded since the \n",
      "program launched in 2018 \n",
      "Supported with funding from the Province of \n",
      "Ontario, the Vector Institute Scholarship in Artifcial \n",
      "Intelligence (VSAI) helps Ontario universities to attract \n",
      "the best and brightest students to study in AI-related \n",
      "master‚Äôs programs. \n",
      "Scholarship recipients connect directly with leading\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "5 \n",
      "Annual Report 2021‚Äì22Vector Institute\n",
      "SPOTLIGHT ON FIVE YEARS OF AI \n",
      "LEADERSHIP FOR CANADIANS \n",
      "SINCE THE VECTOR INSTITUTE WAS FOUNDED IN 2017: \n",
      "2,080+ \n",
      "Students have graduated from \n",
      "Vector-recognized AI programs and \n",
      "study paths \n",
      "$6.2 M \n",
      "Scholarship funds committed to \n",
      "students in AI programs \n",
      "3,700+ \n",
      "Postings for AI-focused jobs and \n",
      "internships ofered on Vector‚Äôs \n",
      "Digital Talent Hub \n",
      "$103 M \n",
      "In research funding committed to \n",
      "Vector-afliated researchers \n",
      "94 \n",
      "Research awards earned by\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "my professional and academic journey.‚Äù \n",
      "Alex Cui, Vector Scholarship in AI Recipient 2021‚Äì22 \n",
      "‚ÄúThe scholarship funding from the Vector Institute \n",
      "has played an instrumental role in expanding \n",
      "graduate teaching, learning, and research \n",
      "opportunities in AI at Queen‚Äôs University.‚Äù \n",
      "Dr. Fahim Quadir, Vice-Provost and Dean, School of \n",
      "Graduate Studies & Professor of Global Developmental \n",
      "Studies, Queen‚Äôs University \n",
      "PRACTICAL, HANDS-ON \n",
      "PROGRAMMING TO FOSTER \n",
      "WORKFORCE SKILLS \n",
      "AND EXPERIENCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "25 \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "VECTOR RECOGNIZED UNIVERSITY \n",
      "PROGRAMS ARE BUILDING ONTARIO‚ÄôS \n",
      "AI WORKFORCE OF THE FUTURE \n",
      "26 \n",
      "Recognized AI master‚Äôs programs  \n",
      "at 11 Ontario universities \n",
      "7 New degree programs \n",
      "19 Programs with updated curricula in \n",
      "AI-specifc minors, concentrations \n",
      "and courses \n",
      "A\n",
      "s part of its work to help develop a steady pipeline of AI-skilled talent, \n",
      "Vector awards recognition to AI master‚Äôs programs and AI-focused\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "23 \n",
      "RESEARCH AWARDS AND \n",
      "ACHIEVEMENTS \n",
      "Each year, members of Vector‚Äôs research community \n",
      "are recognized for outstanding contributions to AI and \n",
      "machine learning felds. Highlights of 2021‚Äì22 include: \n",
      "GLOBAL REACH OF VECTOR \n",
      "RESEARCHERS AND THEIR WORK \n",
      "Vector researchers published papers, gave \n",
      "presentations, or led workshops at many of the \n",
      "top AI conferences this year, including NeurIPS, \n",
      "CVPR, ICLR, ICML, and ACM FAccT. \n",
      "380+ Research papers presented at  \n",
      "high-impact global \n",
      "conferences and in top-\n"
     ]
    }
   ],
   "source": [
    "# Print the retrieved documents\n",
    "pretty_print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008507b",
   "metadata": {},
   "source": [
    "## Now send the query to the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23499f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "\n",
      "109 Vector Scholarships in AI were awarded in 2022.\n"
     ]
    }
   ],
   "source": [
    "query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "result = query_engine.query(query)\n",
    "print(f\"Result: \\n\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb632b45-b135-4561-9759-99fcc03e6959",
   "metadata": {},
   "source": [
    "The model provides the correct answer (109) using the retrieved information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
